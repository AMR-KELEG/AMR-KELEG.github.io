---
---

@inproceedings{keleg-etal-2024-estimating,
    title = "Estimating the Level of Dialectness Predicts Inter-annotator Agreement in Multi-dialect {A}rabic Datasets",
    author = "Keleg, Amr  and
      Magdy, Walid  and
      Goldwater, Sharon",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-short.69",
    html = "https://aclanthology.org/2024.acl-short.69",
    pages = "766--777",
    abstract = "On annotating multi-dialect Arabic datasets, it is common to randomly assign the samples across a pool of native Arabic speakers. Recent analyses recommended routing dialectal samples to native speakers of their respective dialects to build higher-quality datasets. However, automatically identifying the dialect of samples is hard. Moreover, the pool of annotators who are native speakers of specific Arabic dialects might be scarce. Arabic Level of Dialectness (ALDi) was recently introduced as a quantitative variable that measures how sentences diverge from Standard Arabic. On randomly assigning samples to annotators, we hypothesize that samples of higher ALDi scores are harder to label especially if they are written in dialects that the annotators do not speak. We test this by analyzing the relation between ALDi scores and the annotators{'} agreement, on 15 public datasets having raw individual sample annotations for various sentence-classification tasks. We find strong evidence supporting our hypothesis for 11 of them. Consequently, we recommend prioritizing routing samples of high ALDi scores to native speakers of each sample{'}s dialect, for which the dialect could be automatically identified at higher accuracies.",
    selected={true},
    bibtex_show={true},
    award={ACL 2024 Outstanding Paper Award},
    award_name={ACL 2024 Outstanding Paper Award},
}

@inproceedings{abdul-mageed-etal-2024-nadi,
    title = "{NADI} 2024: The Fifth Nuanced {A}rabic Dialect Identification Shared Task",
    author = "Abdul-Mageed, Muhammad  and
      Keleg, Amr  and
      Elmadany, AbdelRahim  and
      Zhang, Chiyu  and
      Hamed, Injy  and
      Magdy, Walid  and
      Bouamor, Houda  and
      Habash, Nizar",
    editor = "Habash, Nizar  and
      Bouamor, Houda  and
      Eskander, Ramy  and
      Tomeh, Nadi  and
      Abu Farha, Ibrahim  and
      Abdelali, Ahmed  and
      Touileb, Samia  and
      Hamed, Injy  and
      Onaizan, Yaser  and
      Alhafni, Bashar  and
      Antoun, Wissam  and
      Khalifa, Salam  and
      Haddad, Hatem  and
      Zitouni, Imed  and
      AlKhamissi, Badr  and
      Almatham, Rawan  and
      Mrini, Khalil",
    booktitle = "Proceedings of The Second Arabic Natural Language Processing Conference",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.arabicnlp-1.79",
    html = "https://aclanthology.org/2024.arabicnlp-1.79",
    pages = "709--728",
    abstract = "We describe the findings of the fifth Nuanced Arabic Dialect Identification Shared Task (NADI 2024). NADI{'}s objective is to help advance SoTA Arabic NLP by providing guidance, datasets, modeling opportunities, and standardized evaluation conditions that allow researchers to collaboratively compete on prespecified tasks. NADI 2024 targeted both dialect identification cast as a multi-label task (Subtask 1), identification of the Arabic level of dialectness (Subtask 2), and dialect-to-MSA machine translation (Subtask 3). A total of 51 unique teams registered for the shared task, of whom 12 teams have participated (with 76 valid submissions during the test phase). Among these, three teams participated in Subtask 1, three in Subtask 2, and eight in Subtask 3. The winning teams achieved 50.57 F1 on Subtask 1, 0.1403 RMSE for Subtask 2, and 20.44 BLEU in Subtask 3, respectively. Results show that Arabic dialect processing tasks such as dialect identification and machine translation remain challenging. We describe the methods employed by the participating teams and briefly offer an outlook for NADI.",
    selected={true},
    bibtex_show={true}
}


@inproceedings{keleg-etal-2023-aldi,
    title = "{ALD}i: Quantifying the {A}rabic Level of Dialectness of Text",
    author = "Keleg, Amr  and
      Goldwater, Sharon  and
      Magdy, Walid",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    selected={true},
    bibtex_show={true},
    code = "https://github.com/AMR-KELEG/ALDi",
    poster = "ALDi_poster.pdf",
    arxiv="2310.13747",
    website="https://huggingface.co/spaces/AMR-KELEG/ALDi",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.655",
    html = "https://aclanthology.org/2023.emnlp-main.655",
    doi = "10.18653/v1/2023.emnlp-main.655",
    pages = "10597--10611",
    abstract = "Transcribed speech and user-generated text in Arabic typically contain a mixture of Modern Standard Arabic (MSA), the standardized language taught in schools, and Dialectal Arabic (DA), used in daily communications. To handle this variation, previous work in Arabic NLP has focused on Dialect Identification (DI) on the sentence or the token level. However, DI treats the task as binary, whereas we argue that Arabic speakers perceive a spectrum of dialectness, which we operationalize at the sentence level as the Arabic Level of Dialectness (ALDi), a continuous linguistic variable. We introduce the AOC-ALDi dataset (derived from the AOC dataset), containing 127,835 sentences (17{\%} from news articles and 83{\%} from user comments on those articles) which are manually labeled with their level of dialectness. We provide a detailed analysis of AOC-ALDi and show that a model trained on it can effectively identify levels of dialectness on a range of other corpora (including dialects and genres not included in AOC-ALDi), providing a more nuanced picture than traditional DI systems. Through case studies, we illustrate how ALDi can reveal Arabic speakers{'} stylistic choices in different situations, a useful property for sociolinguistic analyses.",
}

@inproceedings{keleg-magdy-2023-arabic,
    title = "{A}rabic Dialect Identification under Scrutiny: Limitations of Single-label Classification",
    author = "Keleg, Amr  and
      Magdy, Walid",
    editor = "Sawaf, Hassan  and
      El-Beltagy, Samhaa  and
      Zaghouani, Wajdi  and
      Magdy, Walid  and
      Abdelali, Ahmed  and
      Tomeh, Nadi  and
      Abu Farha, Ibrahim  and
      Habash, Nizar  and
      Khalifa, Salam  and
      Keleg, Amr  and
      Haddad, Hatem  and
      Zitouni, Imed  and
      Mrini, Khalil  and
      Almatham, Rawan",
    selected={true},
    bibtex_show={true},
    code = "https://github.com/AMR-KELEG/ADI-under-scrutiny",
    arxiv="2310.13661",
    booktitle = "Proceedings of ArabicNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore (Hybrid)",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.arabicnlp-1.31",
    html = "https://aclanthology.org/2023.arabicnlp-1.31",
    doi = "10.18653/v1/2023.arabicnlp-1.31",
    pages = "385--398",
    abstract = "Automatic Arabic Dialect Identification (ADI) of text has gained great popularity since it was introduced in the early 2010s. Multiple datasets were developed, and yearly shared tasks have been running since 2018. However, ADI systems are reported to fail in distinguishing between the micro-dialects of Arabic. We argue that the currently adopted framing of the ADI task as a single-label classification problem is one of the main reasons for that. We highlight the limitation of the incompleteness of the Dialect labels and demonstrate how it impacts the evaluation of ADI systems. A manual error analysis for the predictions of an ADI, performed by 7 native speakers of different Arabic dialects, revealed that $\approx$ 67{\%} of the validated errors are not true errors. Consequently, we propose framing ADI as a multi-label classification task and give recommendations for designing new ADI datasets.",
}

@inproceedings{keleg-magdy-2023-dlama,
    selected={true},
    bibtex_show={true},
    code = "https://github.com/AMR-KELEG/DLAMA",
    poster = "DLAMA_poster.pdf",
    slides = "DLAMA_slides.pdf",
    code = "https://github.com/AMR-KELEG/DLAMA",
    title = "{DLAMA}: A Framework for Curating Culturally Diverse Facts for Probing the Knowledge of Pretrained Language Models",
    author = "Keleg, Amr  and
      Magdy, Walid",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-acl.389",
    html = "https://aclanthology.org/2023.findings-acl.389",
    pages = "6245--6266",
    abstract = "A few benchmarking datasets have been released to evaluate the factual knowledge of pretrained language models. These benchmarks (e.g., LAMA, and ParaRel) are mainly developed in English and later are translated to form new multilingual versions (e.g., mLAMA, and mParaRel). Results on these multilingual benchmarks suggest that using English prompts to recall the facts from multilingual models usually yields significantly better and more consistent performance than using non-English prompts. Our analysis shows that mLAMA is biased toward facts from Western countries, which might affect the fairness of probing models. We propose a new framework for curating factual triples from Wikidata that are culturally diverse. A new benchmark DLAMA-v1 is built of factual triples from three pairs of contrasting cultures having a total of 78,259 triples from 20 relation predicates. The three pairs comprise facts representing the (Arab and Western), (Asian and Western), and (South American and Western) countries respectively. Having a more balanced benchmark (DLAMA-v1) supports that mBERT performs better on Western facts than non-Western ones, while monolingual Arabic, English, and Korean models tend to perform better on their culturally proximate facts. Moreover, both monolingual and multilingual models tend to make a prediction that is culturally or geographically relevant to the correct label, even if the prediction is wrong.",
}


@inproceedings{keleg-magdy-2022-smash,
    selected={true},
    title = "{SMASH} at Qur{'}an {QA} 2022: Creating Better Faithful Data 
Splits for Low-resourced Question Answering Scenarios",
    author = "Keleg, Amr  and
      Magdy, Walid",
    booktitle = "Proceedings of the 5th Workshop on Open-Source Arabic 
Corpora and Processing Tools with Shared Tasks on Qur'an QA and 
Fine-Grained Hate Speech Detection",
    month = jun,
    year = "2022",
    slides = "smash_QA_slides.pdf",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://aclanthology.org/2022.osact-1.17",
    html = "https://aclanthology.org/2022.osact-1.17",
    pages = "136--145",
    abstract = "The Qur{'}an QA 2022 shared task aims at assessing the 
possibility of building systems that can extract answers to religious 
questions given relevant passages from the Holy Qur{'}an. This paper 
describes SMASH{'}s system that was used to participate in this shared 
task. Our experiments reveal a data leakage issue among the different 
splits of the dataset. This leakage problem hinders the reliability of 
using the models{'} performance on the development dataset as a proxy for 
the ability of the models to generalize to new unseen samples. After 
creating better faithful splits from the original dataset, the basic 
strategy of fine-tuning a language model pretrained on classical Arabic 
text yielded the best performance on the new evaluation split. The results 
achieved by the model suggests that the small scale dataset is not enough 
to fine-tune large transformer-based language models in a way that 
generalizes well. Conversely, we believe that further attention could be 
paid to the type of questions that are being used to train the models 
given the sensitivity of the data.",
}


@inproceedings{keleg-etal-2022-automatically,
    selected={false},
    title = "Automatically Discarding Straplines to Improve Data Quality for Abstractive News Summarization",
    author = "Keleg, Amr  and
      Lindemann, Matthias  and
      Liu, Danyang  and
      Long, Wanqiu  and
      Webber, Bonnie L.",
    booktitle = "Proceedings of NLP Power! The First Workshop on Efficient Benchmarking in NLP",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.nlppower-1.5",
    html = "https://aclanthology.org/2022.nlppower-1.5",
    pages = "42--51",
    abstract = "Recent improvements in automatic news summarization fundamentally rely on large corpora of news articles and their 
summaries. These corpora are often constructed by scraping news websites, which results in including not only summaries but also other 
kinds of texts. Apart from more generic noise, we identify straplines as a form of text scraped from news websites that commonly turn out 
not to be summaries. The presence of these non-summaries threatens the validity of scraped corpora as benchmarks for news summarization. 
We have annotated extracts from two news sources that form part of the Newsroom corpus (Grusky et al., 2018), labeling those which were 
straplines, those which were summaries, and those which were both. We present a rule-based strapline detection method that achieves good 
performance on a manually annotated test set. Automatic evaluation indicates that removing straplines and noise from the training data of 
a news summarizer results in higher quality summaries, with improvements as high as 7 points ROUGE score.",
}


@inproceedings{keleg-etal-2020-unsupervised,
    selected={false},
    title = "An Unsupervised Method for Weighting Finite-state Morphological Analyzers",
    author = "Keleg, Amr  and
      Tyers, Francis  and
      Howell, Nick  and
      Pirinen, Tommi",
    booktitle = "Proceedings of the 12th Language Resources and Evaluation Conference",
    month = may,
    year = "2020",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://www.aclweb.org/anthology/2020.lrec-1.474",
    html = "https://www.aclweb.org/anthology/2020.lrec-1.474/",
    pages = "3842--3850",
    abstract = "Morphological analysis is one of the tasks that have been studied for years. Different techniques have been used to develop models for performing morphological analysis. Models based on finite state transducers have proved to be more suitable for languages with low available resources. In this paper, we have developed a method for weighting a morphological analyzer built using finite state transducers in order to disambiguate its results. The method is based on a word2vec model that is trained in a completely unsupervised way using raw untagged corpora and is able to capture the semantic meaning of the words. Most of the methods used for disambiguating the results of a morphological analyzer relied on having tagged corpora that need to manually built. Additionally, the method developed uses information about the token irrespective of its context unlike most of the other techniques that heavily rely on the word{'}s context to disambiguate its set of candidate analyses.",
    language = "English",
    ISBN = "979-10-95546-34-4",
}

@inproceedings{keleg-etal-2020-asu,
    selected={false},
    title = "{ASU}{\_}{OPTO} at {OSACT}4 - Offensive Language Detection for {A}rabic text",
    author = "Keleg, Amr  and
      El-Beltagy, Samhaa R.  and
      Khalil, Mahmoud",
    booktitle = "Proceedings of the 4th Workshop on Open-Source Arabic Corpora and Processing Tools, with a Shared Task on Offensive Language Detection",
    month = may,
    year = "2020",
    address = "Marseille, France",
    publisher = "European Language Resource Association",
    url = "https://www.aclweb.org/anthology/2020.osact-1.10",
    html= "https://www.aclweb.org/anthology/2020.osact-1.10/",
    pages = "66--70",
    abstract = "In the past years, toxic comments and offensive speech are polluting the internet and manual inspection of these comments is becoming a tiresome task to manage. Having a machine learning based model that is able to filter offensive Arabic content is of high need nowadays. In this paper, we describe the model that was submitted to the Shared Task on Offensive Language Detection that is organized by (The 4th Workshop on Open-Source Arabic Corpora and Processing Tools). Our model makes use transformer based model (BERT) to detect offensive content. We came in the fourth place in subtask A (detecting Offensive Speech) and in the third place in subtask B (detecting Hate Speech).",
    language = "English",
    ISBN = "979-10-95546-51-1",
}
