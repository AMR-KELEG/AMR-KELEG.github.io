---
---

@misc{keleg2023aldi,
    selected={true},
    bibtex_show={true},
    code = "https://github.com/AMR-KELEG/ALDi",
    arxiv="2310.13747",
    website="https://huggingface.co/spaces/AMR-KELEG/ALDi",
    title={ALDi: Quantifying the Arabic Level of Dialectness of Text},
    author={Amr Keleg and Sharon Goldwater and Walid Magdy},
    year={2023},
    eprint={2310.13747},
    archivePrefix={arXiv},
    primaryClass={cs.CL},
    abstract = "Transcribed speech and user-generated text in Arabic typically contain a mixture of Modern Standard Arabic (MSA), the standardized language taught in schools, and Dialectal Arabic (DA), used in daily communications. To handle this variation, previous work in Arabic NLP has focused on Dialect Identification (DI) on the sentence or the token level. However, DI treats the task as binary, whereas we argue that Arabic speakers perceive a spectrum of dialectness, which we operationalize at the sentence level as the Arabic Level of Dialectness (ALDi), a continuous linguistic variable. We introduce the AOC-ALDi dataset (derived from the AOC dataset), containing 127,835 sentences (17% from news articles and 83% from user comments on those articles) which are manually labeled with their level of dialectness. We provide a detailed analysis of AOC-ALDi and show that a model trained on it can effectively identify levels of dialectness on a range of other corpora (including dialects and genres not included in AOC-ALDi), providing a more nuanced picture than traditional DI systems. Through case studies, we illustrate how ALDi can reveal Arabic speakers' stylistic choices in different situations, a useful property for sociolinguistic analyses."
}

@inproceedings{keleg-magdy-2023-dlama,
    selected={true},
    bibtex_show={true},
    code = "https://github.com/AMR-KELEG/DLAMA",
    poster = "DLAMA_poster.pdf",
    slides = "DLAMA_slides.pdf",
    code = "https://github.com/AMR-KELEG/DLAMA",
    title = "{DLAMA}: A Framework for Curating Culturally Diverse Facts for Probing the Knowledge of Pretrained Language Models",
    author = "Keleg, Amr  and
      Magdy, Walid",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-acl.389",
    html = "https://aclanthology.org/2023.findings-acl.389",
    pages = "6245--6266",
    abstract = "A few benchmarking datasets have been released to evaluate the factual knowledge of pretrained language models. These benchmarks (e.g., LAMA, and ParaRel) are mainly developed in English and later are translated to form new multilingual versions (e.g., mLAMA, and mParaRel). Results on these multilingual benchmarks suggest that using English prompts to recall the facts from multilingual models usually yields significantly better and more consistent performance than using non-English prompts. Our analysis shows that mLAMA is biased toward facts from Western countries, which might affect the fairness of probing models. We propose a new framework for curating factual triples from Wikidata that are culturally diverse. A new benchmark DLAMA-v1 is built of factual triples from three pairs of contrasting cultures having a total of 78,259 triples from 20 relation predicates. The three pairs comprise facts representing the (Arab and Western), (Asian and Western), and (South American and Western) countries respectively. Having a more balanced benchmark (DLAMA-v1) supports that mBERT performs better on Western facts than non-Western ones, while monolingual Arabic, English, and Korean models tend to perform better on their culturally proximate facts. Moreover, both monolingual and multilingual models tend to make a prediction that is culturally or geographically relevant to the correct label, even if the prediction is wrong.",
}


@inproceedings{keleg-magdy-2022-smash,
    selected={true},
    title = "{SMASH} at Qur{'}an {QA} 2022: Creating Better Faithful Data 
Splits for Low-resourced Question Answering Scenarios",
    author = "Keleg, Amr  and
      Magdy, Walid",
    booktitle = "Proceedings of the 5th Workshop on Open-Source Arabic 
Corpora and Processing Tools with Shared Tasks on Qur'an QA and 
Fine-Grained Hate Speech Detection",
    month = jun,
    year = "2022",
    slides = "smash_QA_slides.pdf",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://aclanthology.org/2022.osact-1.17",
    html = "https://aclanthology.org/2022.osact-1.17",
    pages = "136--145",
    abstract = "The Qur{'}an QA 2022 shared task aims at assessing the 
possibility of building systems that can extract answers to religious 
questions given relevant passages from the Holy Qur{'}an. This paper 
describes SMASH{'}s system that was used to participate in this shared 
task. Our experiments reveal a data leakage issue among the different 
splits of the dataset. This leakage problem hinders the reliability of 
using the models{'} performance on the development dataset as a proxy for 
the ability of the models to generalize to new unseen samples. After 
creating better faithful splits from the original dataset, the basic 
strategy of fine-tuning a language model pretrained on classical Arabic 
text yielded the best performance on the new evaluation split. The results 
achieved by the model suggests that the small scale dataset is not enough 
to fine-tune large transformer-based language models in a way that 
generalizes well. Conversely, we believe that further attention could be 
paid to the type of questions that are being used to train the models 
given the sensitivity of the data.",
}


@inproceedings{keleg-etal-2022-automatically,
    selected={false},
    title = "Automatically Discarding Straplines to Improve Data Quality for Abstractive News Summarization",
    author = "Keleg, Amr  and
      Lindemann, Matthias  and
      Liu, Danyang  and
      Long, Wanqiu  and
      Webber, Bonnie L.",
    booktitle = "Proceedings of NLP Power! The First Workshop on Efficient Benchmarking in NLP",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.nlppower-1.5",
    html = "https://aclanthology.org/2022.nlppower-1.5",
    pages = "42--51",
    abstract = "Recent improvements in automatic news summarization fundamentally rely on large corpora of news articles and their 
summaries. These corpora are often constructed by scraping news websites, which results in including not only summaries but also other 
kinds of texts. Apart from more generic noise, we identify straplines as a form of text scraped from news websites that commonly turn out 
not to be summaries. The presence of these non-summaries threatens the validity of scraped corpora as benchmarks for news summarization. 
We have annotated extracts from two news sources that form part of the Newsroom corpus (Grusky et al., 2018), labeling those which were 
straplines, those which were summaries, and those which were both. We present a rule-based strapline detection method that achieves good 
performance on a manually annotated test set. Automatic evaluation indicates that removing straplines and noise from the training data of 
a news summarizer results in higher quality summaries, with improvements as high as 7 points ROUGE score.",
}


@inproceedings{keleg-etal-2020-unsupervised,
    selected={false},
    title = "An Unsupervised Method for Weighting Finite-state Morphological Analyzers",
    author = "Keleg, Amr  and
      Tyers, Francis  and
      Howell, Nick  and
      Pirinen, Tommi",
    booktitle = "Proceedings of the 12th Language Resources and Evaluation Conference",
    month = may,
    year = "2020",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://www.aclweb.org/anthology/2020.lrec-1.474",
    html = "https://www.aclweb.org/anthology/2020.lrec-1.474/",
    pages = "3842--3850",
    abstract = "Morphological analysis is one of the tasks that have been studied for years. Different techniques have been used to develop models for performing morphological analysis. Models based on finite state transducers have proved to be more suitable for languages with low available resources. In this paper, we have developed a method for weighting a morphological analyzer built using finite state transducers in order to disambiguate its results. The method is based on a word2vec model that is trained in a completely unsupervised way using raw untagged corpora and is able to capture the semantic meaning of the words. Most of the methods used for disambiguating the results of a morphological analyzer relied on having tagged corpora that need to manually built. Additionally, the method developed uses information about the token irrespective of its context unlike most of the other techniques that heavily rely on the word{'}s context to disambiguate its set of candidate analyses.",
    language = "English",
    ISBN = "979-10-95546-34-4",
}

@inproceedings{keleg-etal-2020-asu,
    selected={true},
    title = "{ASU}{\_}{OPTO} at {OSACT}4 - Offensive Language Detection for {A}rabic text",
    author = "Keleg, Amr  and
      El-Beltagy, Samhaa R.  and
      Khalil, Mahmoud",
    booktitle = "Proceedings of the 4th Workshop on Open-Source Arabic Corpora and Processing Tools, with a Shared Task on Offensive Language Detection",
    month = may,
    year = "2020",
    address = "Marseille, France",
    publisher = "European Language Resource Association",
    url = "https://www.aclweb.org/anthology/2020.osact-1.10",
    html= "https://www.aclweb.org/anthology/2020.osact-1.10/",
    pages = "66--70",
    abstract = "In the past years, toxic comments and offensive speech are polluting the internet and manual inspection of these comments is becoming a tiresome task to manage. Having a machine learning based model that is able to filter offensive Arabic content is of high need nowadays. In this paper, we describe the model that was submitted to the Shared Task on Offensive Language Detection that is organized by (The 4th Workshop on Open-Source Arabic Corpora and Processing Tools). Our model makes use transformer based model (BERT) to detect offensive content. We came in the fourth place in subtask A (detecting Offensive Speech) and in the third place in subtask B (detecting Hate Speech).",
    language = "English",
    ISBN = "979-10-95546-51-1",
}
